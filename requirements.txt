# -------------------------------------------------
# Core libraries required by curate_ner_datasets.py
# -------------------------------------------------
datasets==2.19.0          # ðŸ¤— Datasets â€“ data loading & preprocessing
tqdm==4.66.5              # progress bars
pandas==2.2.2             # optional â€“ only used for the MITâ€‘R placeholder routine
requests==2.32.3          # HTTP GET for the MITâ€‘R CSV download
huggingface_hub==0.24.0    # needed by ðŸ¤— Datasets to fetch from the Hub
# -------------------------------------------------
# Optional â€“ only if you plan to run inference / fineâ€‘tune afterwards
# -------------------------------------------------
# torch==2.3.0               # CPUâ€‘only build (no CUDA)
# torch==2.3.0+cu121        # CUDAâ€¯12.1 (replace 121 with your CUDA version)
#   â€“ install from the PyTorch wheel index, e.g.:
#   pip install torch==2.3.0+cu121 -f https://download.pytorch.org/whl/torch_stable.html
# -------------------------------------------------
# Extras you might find handy while developing
# -------------------------------------------------
peft==0.10.0               # LoRA / PEFT utilities (used in later training scripts)
accelerate==0.30.1         # optional â€“ for distributed / mixedâ€‘precision training
# -------------------------------------------------
# Core ML and Training Dependencies
torch>=2.0.0
transformers>=4.36.0
datasets>=2.14.0
accelerate>=0.24.0
peft>=0.7.0
tokenizers>=0.15.0

# Training and Experimentation
wandb>=0.16.0
tensorboard>=2.14.0

# Data Processing and Analysis
pandas>=2.0.0
numpy>=1.24.0
scikit-learn>=1.3.0

# Visualization
matplotlib>=3.7.0
seaborn>=0.12.0

# Utilities
tqdm>=4.65.0
psutil>=5.9.0
gpustat>=1.1.0

# Optional: For advanced features
# flash-attn>=2.3.0  # Uncomment if using flash attention
# bitsandbytes>=0.41.0  # Uncomment if using 4-bit quantization