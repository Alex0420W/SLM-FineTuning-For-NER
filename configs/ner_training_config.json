{
  "dataset_info": {
    "total_samples": 399,
    "entity_types": [
      "B-LOC",
      "B-ORG",
      "I-ORG",
      "B-MISC",
      "B-PER",
      "I-PER",
      "I-LOC",
      "I-MISC"
    ],
    "entity_distribution": {
      "B-LOC": 172,
      "B-ORG": 211,
      "I-ORG": 288,
      "B-MISC": 52,
      "B-PER": 156,
      "I-PER": 193,
      "I-LOC": 142,
      "I-MISC": 13
    },
    "unique_entity_types": 8
  },
  "training_config": {
    "task_type": "named_entity_recognition",
    "model_recommendations": [
      "microsoft/Phi-3.5-mini-instruct",
      "HuggingFaceTB/SmolLM-1.7B-Instruct",
      "Qwen/Qwen2-1.5B-Instruct",
      "google/gemma-2-2b-it"
    ],
    "training_args": {
      "per_device_train_batch_size": 4,
      "per_device_eval_batch_size": 4,
      "learning_rate": 2e-05,
      "num_train_epochs": 3,
      "warmup_ratio": 0.1,
      "logging_steps": 10,
      "eval_steps": 100,
      "save_steps": 500,
      "fp16": true,
      "gradient_checkpointing": true
    },
    "lora_config": {
      "r": 16,
      "lora_alpha": 32,
      "target_modules": [
        "q_proj",
        "v_proj",
        "k_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj"
      ],
      "lora_dropout": 0.1,
      "bias": "none",
      "task_type": "CAUSAL_LM"
    }
  }
}