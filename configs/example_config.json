{
    "model_name": "microsoft/Phi-3.5-mini-instruct",
    "max_length": 512,
    "dataset_paths": {
        "train": "data/train_ner.jsonl",
        "val": "data/val_ner.jsonl",
        "test": "data/test_ner.jsonl"
    },
    "output_dir": "./models/ner_finetuned_model",
    "training_args": {
        "per_device_train_batch_size": 4,
        "per_device_eval_batch_size": 4,
        "learning_rate": 2e-5,
        "num_train_epochs": 3,
        "warmup_ratio": 0.1,
        "logging_steps": 10,
        "eval_steps": 100,
        "save_steps": 500,
        "evaluation_strategy": "steps",
        "save_strategy": "steps",
        "load_best_model_at_end": true,
        "metric_for_best_model": "eval_loss",
        "greater_is_better": false,
        "fp16": true,
        "gradient_checkpointing": true,
        "dataloader_pin_memory": false,
        "remove_unused_columns": false,
        "report_to": "wandb"
    },
    "lora_config": {
        "r": 16,
        "lora_alpha": 32,
        "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        "lora_dropout": 0.1,
        "bias": "none",
        "task_type": "CAUSAL_LM"
    },
    "generation_config": {
        "max_new_tokens": 150,
        "temperature": 0.1,
        "do_sample": false,
        "pad_token_id": null
    }
}
